{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Karpathy Video\n",
    "https://youtu.be/zduSFxRajkE\n",
    "\n",
    "Tokenizer Webapp\n",
    "https://tiktokenizer.vercel.app/\n",
    "\n",
    "- non english text tends to have more tokens per sentence than english which explodes the context.\n",
    "- whitespace tokenization has a similar \"bloating the context\" effect for leading whitespaces in code.\n",
    "- gpt4 tokenizer grouped whitespaces, effectively \"densifying\" python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unicode of any character\n",
    "ord('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S 83\n",
      "e 101\n",
      "n 110\n",
      "t 116\n",
      "e 101\n",
      "n 110\n",
      "c 99\n",
      "e 101\n",
      "  32\n",
      "w 119\n",
      "i 105\n",
      "t 116\n",
      "h 104\n",
      "  32\n",
      "a 97\n",
      "n 110\n",
      "  32\n",
      "e 101\n",
      "m 109\n",
      "o 111\n",
      "j 106\n",
      "i 105\n",
      "  32\n",
      "ðŸ˜Š 128522\n"
     ]
    }
   ],
   "source": [
    "for c in \"Sentence with an emoji ðŸ˜Š\":\n",
    "    print(c, ord(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UTF-8 is the most prefered encoding, and is the only encoding that is backwards compatible to ASCII\n",
    "- utf8 encodes in one to four bytes\n",
    "- utf16 and utf32 are very wasteful in terms of encoding\n",
    "- utf8 has a very small vocabulary size, which would \"stretch\" the context very far, which won't allow us to attend to sufficiently large context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[115,\n",
       " 101,\n",
       " 110,\n",
       " 116,\n",
       " 101,\n",
       " 110,\n",
       " 99,\n",
       " 101,\n",
       " 32,\n",
       " 119,\n",
       " 105,\n",
       " 116,\n",
       " 104,\n",
       " 32,\n",
       " 97,\n",
       " 110,\n",
       " 32,\n",
       " 101,\n",
       " 109,\n",
       " 111,\n",
       " 106,\n",
       " 105,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 152,\n",
       " 138]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"sentence with an emoji ðŸ˜Š\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BPE will iteratively find the pair of tokens that occur most frequently and merge them into a single token. This is done iteratively until the vocabulary size is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw text of size 43\n",
      "this is an example text, here is an emoji ðŸ˜Š\n",
      "encoded text of size 46\n",
      "[116, 104, 105, 115, 32, 105, 115, 32, 97, 110, 32, 101, 120, 97, 109, 112, 108, 101, 32, 116, 101, 120, 116, 44, 32, 104, 101, 114, 101, 32, 105, 115, 32, 97, 110, 32, 101, 109, 111, 106, 105, 32, 240, 159, 152, 138]\n"
     ]
    }
   ],
   "source": [
    "text = \"this is an example text, here is an emoji ðŸ˜Š\"\n",
    "tokens = [int(t) for t in text.encode('utf-8')]\n",
    "print(f\"raw text of size {len(text)}\")\n",
    "print(text)\n",
    "print(f\"encoded text of size {len(tokens)}\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((105, 115), 3), ((115, 32), 3), ((32, 105), 2), ((32, 97), 2), ((97, 110), 2), ((110, 32), 2), ((32, 101), 2), ((101, 120), 2), ((101, 32), 2), ((116, 104), 1), ((104, 105), 1), ((120, 97), 1), ((97, 109), 1), ((109, 112), 1), ((112, 108), 1), ((108, 101), 1), ((32, 116), 1), ((116, 101), 1), ((120, 116), 1), ((116, 44), 1), ((44, 32), 1), ((32, 104), 1), ((104, 101), 1), ((101, 114), 1), ((114, 101), 1), ((101, 109), 1), ((109, 111), 1), ((111, 106), 1), ((106, 105), 1), ((105, 32), 1), ((32, 240), 1), ((240, 159), 1), ((159, 152), 1), ((152, 138), 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def pair_frequency(tokens: list):\n",
    "    pairs = {}\n",
    "    for i in range(len(tokens) - 1):\n",
    "        pair = (tokens[i], tokens[i + 1])\n",
    "        if pair in pairs:\n",
    "            pairs[pair] += 1\n",
    "        else:\n",
    "            pairs[pair] = 1\n",
    "    sorted_pairs = sorted(pairs.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_pairs\n",
    "\n",
    "print(pair_frequency(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is some sweetspot between vocabulary size and sequence length\n",
    "- gpt4 uses roughly 100k token vocabulary\n",
    "- tokenizer has its own training set which is used to determine the vocabulary\n",
    "- tokenizer training set has a different mixture than the model training set\n",
    "- unicode apostrophe causes issues: ` vs '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(token_ids: list, vocab: dict):\n",
    "    tokens = [vocab[t] for t in token_ids]\n",
    "    text = b\"\".join(tokens).decode('utf-8', errors='replace') # openai also uses error replace\n",
    "    return text\n",
    "\n",
    "def encode(text: str, vocab: dict):\n",
    "    bytes = list(text.encode('utf-8'))\n",
    "    # have to re-merge the pairs in the order they were split\n",
    "    for _, pair in pair_frequency(bytes):\n",
    "        pass\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken # The openai tokenizer\n",
    "\n",
    "# in the gpt4 tokenizer they changed the regex used to chunk the text before bpe\n",
    "# numbers are not merged longer than 3 digits\n",
    "# byte-encode > encode > decode > byte-decode\n",
    "# tiktoken library is implemented in rust\n",
    "\n",
    "# special token <|im_start|> <|im_end|> stand for \"imaginary monologue start\" and \"imaginary monologue end\"\n",
    "# Adding special tokens will require \"model surgery\" where you have to add an extra row to the embedding table and extend the classification head at the very end by one\n",
    "\n",
    "# The two layers that change with vocab size\n",
    "self.token_embedding_table = nn.Embedding(vocab_size, emb_size)\n",
    "self.lm_head = nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "# when you want to add new tokens for instruction tuning, or \"using the browser\"\n",
    "# you freeze the base model and only push gradients into the layers above\n",
    "\n",
    "# trailing white spaces heavily limit the next token prediction because most tokens have a leading whitespace\n",
    "# SolidGoldMagikarp is a reddit username that was in the tokenization dataset, so it got merged into a token\n",
    "# but because it doesn't appear in the training dataset, it never receives gradients and thus the embedding is a randomly initialized vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentencepiece is used for llama and mistral\n",
    "# unk token is the \"unknown\"\n",
    "# sentencepiece has alot of \"legacy baggage\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
